{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354ca6e6-eb53-4957-b4da-b6a7f2040991",
   "metadata": {},
   "source": [
    "# Handling of sequencing reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa39ee-9fe1-4bb2-9e07-a551163cfaeb",
   "metadata": {},
   "source": [
    "Reads are typically available in [fastq](https://en.wikipedia.org/wiki/FASTQ_format) format after conducting sequencing experiments (Illumina, Nanopore, PacBio).\n",
    "\n",
    "Our analysis will be done in the **Compute Canada (CC)** cluster of analysis.\n",
    "To login into the cluster, follow the instructions available in the [wiki page for new users](https://docs.alliancecan.ca/wiki/SSH). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d8964-9dc9-4ace-9491-1b7d3c3e0f3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring the data\n",
    "\n",
    "Data may be shared through a folder or downloaded from a biorepository. The initial step must be to make it available for yourself in the cluster. \n",
    "\n",
    "The dataset is available in a shared directory so you can move it to the folder you desire using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb4962a-83bd-4004-a43f-436e0a47461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: overwrite './bronch_fastq.tar.gz'? "
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# replace the location with your folder\n",
    "cd /project/6056895/mdprieto/tutorials\n",
    "\n",
    "cp -ri /project/6007413/globus_share/Bronchiectasis_genomes/bronch_fastq.tar.gz .\n",
    "\n",
    "# extract the reads \n",
    "tar -zxf bronch_fastq.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515012ad-749f-4f0f-8db4-a36bba3a0427",
   "metadata": {},
   "source": [
    "If you list the files available in your newly created folder, you will see that files are separated into '_R1.fastq' and '_R2.fastq'.\n",
    "This means that the reads were produced with paired ends and a set of them represents a single sample. In this project, we have '_R0.fastq' and these are reads that were already trimmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a57a35a8-99d8-425a-bafe-d8b59822d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronchiectasis_reads/10-C21_TCCGCGAA-TATAGCCT_L004_R0_001.fastq.gz\n",
      "bronchiectasis_reads/10-C21_TCCGCGAA-TATAGCCT_L004_R1_001.fastq.gz\n",
      "bronchiectasis_reads/10-C21_TCCGCGAA-TATAGCCT_L004_R2_001.fastq.gz\n",
      "bronchiectasis_reads/10-C92_TCCGCGAA-TATAGCCT_L001_R0_001.fastq.gz\n",
      "bronchiectasis_reads/10-C92_TCCGCGAA-TATAGCCT_L001_R1_001.fastq.gz\n",
      "bronchiectasis_reads/10-C92_TCCGCGAA-TATAGCCT_L001_R2_001.fastq.gz\n",
      "bronchiectasis_reads/11-C145_TCTCGCGC-TATAGCCT_L001_R0_001.fastq.gz\n",
      "bronchiectasis_reads/11-C145_TCTCGCGC-TATAGCCT_L001_R1_001.fastq.gz\n",
      "bronchiectasis_reads/11-C145_TCTCGCGC-TATAGCCT_L001_R2_001.fastq.gz\n",
      "bronchiectasis_reads/11-C22_TCTCGCGC-TATAGCCT_L004_R0_001.fastq.gz\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!cd /project/6056895/mdprieto/tutorials\n",
    "!ls bronchiectasis_reads/* | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3c59f-2d7d-4fd2-81a5-9a21a19a04f8",
   "metadata": {},
   "source": [
    "## Quality control\n",
    "\n",
    "It is a good practice to organize every project/analysis in an individual directory. You can also use github repositories to synchronize all your results and scripts so other researchers can easily reproduce your results. \n",
    "\n",
    "Also, as a restriction of compute canada, we cannot launch jobs to the processing cluster from directories that belong to the **HOME** and **PROJECT** directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ddec4-742e-4678-ba62-d23ff92d9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a working directory in scratch\n",
    "mkdir /scratch/mdprieto/tutorials\n",
    "cd /scratch/mdprieto/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516e6ea-9b42-4e7a-b456-c414ecbe1209",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Our pipeline for quality control of raw reads includes several steps:\n",
    "\n",
    "1. We use the ***seqkit*** tool to obtain basic statistics from the **.fastq** files. The module to load **seqkit** is available in CC, so it can be load using `module load`. \n",
    "    - The tool takes `fastq` files as inputs and produces a txt output \n",
    "2. We will use **fastqc** which is also available as a module in ComputeCanada. The tool creates an overall summary of different metrics of sequencing including nucleotide distribution, presence of repeats, quality of base calling, GC content and adapter content\n",
    "    - A for loop lets me apply a command to every file/read available. In this case to all files with suffix 'fastq' in the INPUT_DIR\n",
    "    - **fastqc** produces an output summary for every file entered\n",
    "3. Finally, another tool (**multiqc**) takes all output summaries of **fastqc** in a directory and creates a nice sinfle HTML output that can be visualized in any web browser.\n",
    "    \n",
    "\n",
    "To make it easier to reproduce, we will open a text file with editor `nano` and save the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9af9ee-3480-4fe5-a3a4-663bf6517898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text editor\n",
    "nano tutorial_seqkit.sh\n",
    "\n",
    "# ------------------------------- write following code inside file\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --account=rrg-whsiao-ab                    # compute canada PI allocation\n",
    "#SBATCH --mem=25gb                                 # 25 GB of memory\n",
    "#SBATCH --time=06:00:00\n",
    "#SBATCH --job-name=\"quality_control\"               # name of job\n",
    "#SBATCH --chdir=/scratch/mdprieto/tutorials        # change to directory before start\n",
    "#SBATCH --cpus-per-task=9\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "################################ preparation ######################################\n",
    "\n",
    "# load necessary modules\n",
    "module load StdEnv/2020\n",
    "module load nixpkgs/16.09\n",
    "module load fastqc/0.11.9\n",
    "module load python/3.10.2\n",
    "module load seqkit/0.15.0\n",
    "pip install multiqc\n",
    "\n",
    "# create output directory\n",
    "mkdir -p /scratch/mdprieto/tutorials/qc_results\n",
    "\n",
    "# establish path for output and input\n",
    "OUTPUT_DIR=\"/scratch/mdprieto/tutorials/qc_results\"\n",
    "INPUT_DIR=\"/project/6056895/mdprieto/tutorials/fastq_reads\"\n",
    "\n",
    "################################## seqkit #########################################\n",
    "\n",
    "# run seqkit in fastq file and save output \n",
    "seqkit stats $INPUT_DIR/*.fastq.gz > $OUTPUT_DIR/seqkit_output.txt\n",
    "\n",
    "################################## fastqc #########################################\n",
    "\n",
    "# run fastqc tool for every file in INPUT_DIR\n",
    "for fastq_file in $(ls $INPUT_DIR/*.fastq.gz)\n",
    "do\n",
    "fastqc \\\n",
    "        -o $OUTPUT_DIR \\\n",
    "        -t 9 \\\n",
    "        $fastq_file\n",
    "        # output is saved as individual files in OUTPUT_DIR\n",
    "done\n",
    "\n",
    "################################## multiqc #########################################\n",
    "\n",
    "# move to output dir, multiqc automatically reads all files\n",
    "cd $OUTPUT_DIR\n",
    "multiqc . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397c3e9-7b8a-4c81-9b69-263126cb0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a working directory in scratch\n",
    "mkdir /scratch/mdprieto/tutorials\n",
    "cd /scratch/mdprieto/tutorials\n",
    "\n",
    "# open text editor\n",
    "nano tutorial_seqkit.sh\n",
    "\n",
    "# write following code inside file\n",
    "\n",
    "\n",
    "**fastqc** software produces quality check of reads, all results are synthesized in an `.html` file using **multiqc**\n",
    "\n",
    "``` sh\n",
    "#!/bin/bash\n",
    "#SBATCH --account=rrg-whsiao-ab\n",
    "#SBATCH --mem=25gb # 25 GB of memory\n",
    "#SBATCH --time=06:00:00\n",
    "#SBATCH --job-name=\"quality_control\" # name of job\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --cpus-per-task=9\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "################################ preparation ######################################\n",
    "\n",
    "# load necessary modules\n",
    "module load StdEnv/2020\n",
    "module load nixpkgs/16.09\n",
    "module load fastqc/0.11.9\n",
    "module load python/3.10.2\n",
    "module load seqkit/0.15.0\n",
    "pip install multiqc\n",
    "\n",
    "# establish path for output and input\n",
    "mkdir -p /home/mdprieto/scratch/results_hilliam/fastqc/\n",
    "OUTPUT_DIR=\"/home/mdprieto/scratch/results_hilliam/fastqc_hilliam/\"\n",
    "INPUT_DIR=\"/project/6056895/mdprieto/hilliam_pseudomonas/bronchiectasis_reads\"\n",
    "\n",
    "################################## seqkit #########################################\n",
    "\n",
    "\n",
    "\n",
    "seqkit stats $INPUT_DIR/*.fastq \n",
    "\n",
    "\n",
    "\n",
    "################################## fastqc #########################################\n",
    "\n",
    "for fastq_file in $(ls $INPUT_DIR/*.fastq.gz)\n",
    "do\n",
    "fastqc \\\n",
    "        -o $OUTPUT_DIR \\\n",
    "        -t 9 \\\n",
    "        $fastq_file\n",
    "done\n",
    "\n",
    "################################## multiqc #########################################\n",
    "\n",
    "module load python/3.10.2\n",
    "\n",
    "cd $OUTPUT_DIR\n",
    "multiqc . \n",
    "```\n",
    "\n",
    "## Assembly using spades\n",
    "\n",
    "Create text file with all accession numbers for the study. Each name has corresponding R1, R2 and R0 reads.\n",
    "\n",
    "``` sh\n",
    "cd /project/6056895/mdprieto/hilliam_pseudomonas/bronchiectasis_reads\n",
    "\n",
    "# list files with path and remove suffix\n",
    "find ~/project_mdprieto/hilliam_pseudomonas/bronchiectasis_reads/*fastq.gz | \\\n",
    "sed  's/_R[0-3]_001.fastq.*//' | \\\n",
    "uniq > ~/scratch/hilliam_filenames.txt\n",
    "```\n",
    "\n",
    "Serial job script to run spades on all `.fastq` files. To estimate the runtime, I first use only 19/190 (10%) of samples to run it.\n",
    "\n",
    "``` sh\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --mem=80gb # 80 GB of memory\n",
    "#SBATCH --time=00:50:00\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --job-name=\"spades assembly\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "#################################################\n",
    "######## Preparation\n",
    "\n",
    "# set variables to specify filepaths\n",
    "INPUT_DIR=\"/project/6056895/mdprieto/hilliam_pseudomonas/bronchiectasis_reads/\"\n",
    "OUTPUT_DIR=\"/scratch/mdprieto/results_hilliam/spades\"\n",
    "\n",
    "# load spades module and dependencies\n",
    "module load StdEnv/2020 spades/3.15.3\n",
    "\n",
    "#################################################\n",
    "######## Sample processing\n",
    "\n",
    "for sample in  $(ls $INPUT_DIR*fastq.gz) \n",
    "do\n",
    "\n",
    "# define names of paired end reads inside loop\n",
    "R0=${sample}_R0_001.fastq.gz\n",
    "R1=${sample}_R1_001.fastq.gz\n",
    "R2=${sample}_R2_001.fastq.gz\n",
    "\n",
    "# run spades for each of the 190 samples\n",
    "# --isolate reduces runtime in high coverage genomes\n",
    "# --careful is recommended for illumina technology\n",
    "\n",
    "spades.py \\\n",
    "-1 ${R1} -2 ${R2} -s${R0} \\\n",
    "-t 16 \\\n",
    "--careful \\\n",
    "--cov-cutoff auto \\\n",
    "-o $OUTPUT_DIR\n",
    "done\n",
    "```\n",
    "\n",
    "Pilot script\n",
    "\n",
    "``` sh\n",
    "for sample in  $(cat hilliam_filenames.txt | head -n 19) \n",
    "do\n",
    "# start message\n",
    "echo \"Assembly of $sample\"\n",
    "\n",
    "# define names of paired end reads inside loop\n",
    "R0=${sample}_R0_001.fastq.gz\n",
    "R1=${sample}_R1_001.fastq.gz\n",
    "R2=${sample}_R2_001.fastq.gz\n",
    "\n",
    "# evaluate correct definition of one \n",
    "echo $R0\n",
    "\n",
    "# run spades for each of the 190 samples\n",
    "# --isolate reduces runtime in high coverage genomes\n",
    "# --careful is recommended for illumina technology\n",
    "spades.py \\\n",
    "-1 ${R1} -2 ${R2} -s${R0} \\\n",
    "--careful \\\n",
    " --cov-cutoff auto \\\n",
    "-o $OUTPUT_DIR\n",
    "done\n",
    "```\n",
    "\n",
    "## Assembly pipeline with Shovill\n",
    "\n",
    "Shovill is a tool that optimizes Spades to minimize run time, while maintaining the quality of assembly. See <https://github.com/tseemann/shovill> for more details.\n",
    "\n",
    "We can install from a Docker container. In a HPC, we can create a singularity container from Docker. Singularity is optimized for clusters, Docker modifies root privileges\n",
    "\n",
    "``` sh\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --mem=8gb # 8 GB of memory\n",
    "#SBATCH --time=00:50:00\n",
    "#SBATCH --job-name= singularity build of shovill assembler\n",
    "#SBATCH --chdir= /home/mdprieto/scratch/\n",
    "\n",
    "# load singularity to transform docker container\n",
    "module load singularity/3.8\n",
    "\n",
    "# create singularity container locally\n",
    "singularity build shovill.sif docker://staphb/shovill:latest\n",
    "singularity exec shovill.sif shovill --help\n",
    "\n",
    "# mount my filesystem inside container\n",
    "# ------------------- localscratch is defined to use compute node temp folder\n",
    "singularity run -B /home -B /project -B /scratch -B /localscratch:/temp\n",
    "```\n",
    "\n",
    "After having the singularity container ready, we can assemble our genomes.\n",
    "\n",
    "### Tips to run assembly jobs on Cedar (CC)\n",
    "\n",
    "-   Assembly is a resource intensive job that requires that the data is available in memory for processing. So it is necessary to allocate enough ram per CPU to handle the size of each genome.\n",
    "-   Also, a part of the available memory should be saved (\\~4GB) for additional processes or the OS.\n",
    "-   Finally, bioinformatic procedures usually use multiple threads to optimize performance, so their efficiency increases with the number of available cores.\n",
    "-   In shovill, the `--ram` option specifies the available ram per thread (core)\n",
    "-   **Spades performance increases drastically with the number of threads (--cpus-per-task)**\n",
    "-   Spades will take input of RAM from shovill as total available mem, better to input limit manually with `--opts \"-m XX\"`\n",
    "\n",
    "``` sh\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --mem-per-cpu=12G #  GB of memory per cpu core\n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --ntasks=1 # tasks in parallel\n",
    "#SBATCH --cpus-per-task=16 # CPU cores per task\n",
    "#SBATCH --job-name=\"shovill_assembly_hilliam\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --output=slurm_shovill_16x12.out\n",
    "#SBATCH --mail-user=mprietog@sfu.ca\n",
    "#SBATCH --mail-type=END\n",
    "\n",
    "################################## preparation #########################################\n",
    "\n",
    "# load singularity to execute shovill\n",
    "module purge\n",
    "module load singularity/3.8\n",
    "\n",
    "# mount my filesystem inside container\n",
    "# ---------- localscratch is defined to use compute node temp folder\n",
    "BIND_MOUNT=\"-B /home -B /project -B /scratch -B /localscratch -B /localscratch:/temp\"\n",
    "\n",
    "# create variables and output dir\n",
    "mkdir -p /scratch/mdprieto/results_hilliam/shovill\n",
    "OUTPUT_DIR=\"/scratch/mdprieto/results_hilliam/shovill\"\n",
    "INPUT_DIR=\"/project/6056895/mdprieto/hilliam_pseudomonas/bronchiectasis_reads\"\n",
    "\n",
    "################################## shovill #########################################\n",
    "\n",
    "for file1 in $(ls $INPUT_DIR/*R1*fastq.gz)\n",
    "\n",
    "do\n",
    "    # create environment variables for R2 and R0 files and establish a name for the output directory\n",
    "    file2=${file1/R1/R2}\n",
    "    trimmed=${file1/R1/R0}\n",
    "    out_dir_sample=$(echo $file1 | grep -oE '[0-9]{1,3}-[ABC][0-9]*')\n",
    "\n",
    "    # ------ Execute shovill inside singularity container\n",
    "    # --opts = options to pass into spades assembler\n",
    "    # --ram = total ram in all CPUs\n",
    "\n",
    "    singularity exec $BIND_MOUNT shovill.sif shovill --R1 $file1 --R2 $file2 \\\n",
    "    --outdir $OUTPUT_DIR/$out_dir_sample \\\n",
    "    --opts \"-s $trimmed\" \\\n",
    "    --cpus $SLURM_CPUS_PER_TASK \\\n",
    "    --ram 140 \\\n",
    "    --tmpdir /scratch/mdprieto/tmp\n",
    "    echo \"Finished assembly of sample\"\n",
    "done\n",
    "\n",
    "######################## create new dir with assemblies #############################\n",
    "\n",
    "contigs_dir=\"/scratch/mdprieto/results_hilliam/sample_contigs\"\n",
    "\n",
    "# new directory with sample name appended to the contigs\n",
    "# finds 'contigs.fa' filenames downstream\n",
    "# appends 'sample_name' to each 'contigs.fa' in a new folder 'sample_contigs'\n",
    "\n",
    "mkdir -p $contigs_dir\n",
    "for i in `find /scratch/mdprieto/results_hilliam/shovill -name \"contigs.fa\"`\n",
    "   do cp -n $i $contigs_dir/`echo $i| awk -F/ '{print $6 \"_\" $7}' `\n",
    "done\n",
    "```\n",
    "\n",
    "## QC of assembly\n",
    "\n",
    "### Quast\n",
    "\n",
    "We will use QUAST to generate genome assembly metrics. Before running it, we need to download the reference genome for Pseudomonas aeruginosa (PA1). The output directory is specified with the option `-P`\n",
    "\n",
    "``` sh\n",
    "# genomic fasta\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/496/605/GCF_000496605.2_ASM49660v2/GCF_000496605.2_ASM49660v2_genomic.fna.gz -P /project/6056895/mdprieto/hilliam_pseudomonas/pseudomonas_pa1_reference\n",
    "\n",
    "# genomic coordinates annotation\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/496/605/GCF_000496605.2_ASM49660v2/GCF_000496605.2_ASM49660v2_genomic.gff.gz -P /project/6056895/mdprieto/hilliam_pseudomonas/pseudomonas_pa1_reference\n",
    "```\n",
    "\n",
    "Now, we run quast with and without reference genome. With reference genome we obtain basic assembly measures and metrics of coverage against the curated assembly. Without a reference, we would obtain only the basic measures.\n",
    "\n",
    "``` sh\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --mem-per-cpu=4G #  GB of memory per cpu core\n",
    "#SBATCH --time=00:30:00\n",
    "#SBATCH --ntasks=1 # tasks in parallel\n",
    "#SBATCH --cpus-per-task=8 # CPU cores per task\n",
    "#SBATCH --job-name=\"quast_hilliam\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --output=quast_hilliam.out\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# ----------------------- preparation\n",
    "\n",
    "# load QUAST module and dependencies\n",
    "module load StdEnv/2020 gcc/9.3.0 quast/5.0.2\n",
    "\n",
    "# define internal variables\n",
    "genome_fasta=\"/project/6056895/mdprieto/hilliam_pseudomonas/pseudomonas_pa1_reference/GCF_000496605.2_ASM49660v2_genomic.fna.gz\"\n",
    "genome_gff=\"/project/6056895/mdprieto/hilliam_pseudomonas/pseudomonas_pa1_reference/GCF_000496605.2_ASM49660v2_genomic.gff.gz\"\n",
    "contigs_dir=\"/scratch/mdprieto/results_hilliam/sample_contigs\"\n",
    "output_dir=\"/scratch/mdprieto/results_hilliam/quast\"\n",
    "\n",
    "# ----------------------- quast no reference genome\n",
    "\n",
    "quast.py $contigs_dir/*.fa \\\n",
    "            -r $genome_fasta \\\n",
    "            -g $genome_gff \\\n",
    "            -o $output_dir \\\n",
    "            --threads 7\n",
    "```\n",
    "\n",
    "### CheckM\n",
    "\n",
    "**CheckM** infers the quality of the genome assembly based on the presence and uniqueness of these sets of gene markers. It determines the completeness (coverage of reference genome) and the contamination of the input draft genomes.\n",
    "\n",
    "**CheckM** is not available in the CC cluster. To install it, we create a virtual environment of python in our home directory. After loading the interpreter, we load the `scipy-stack` module that contains necessary python dependencies (matplotlib and numpy). Also, we load a set of bioinformatic tools dependencies (pplacer, prodigal and hmmer).\n",
    "\n",
    "``` sh\n",
    "module load python/3.10.2 scipy-stack\n",
    "module load pplacer/1.1.alpha19 prodigal/2.6.3 hmmer/3.2.1\n",
    "```\n",
    "\n",
    "Is best practice to create virtual environment in your home or project directory. I create `checkm_genome_env` in the home dir; python dependencies are installed after loading environment. The `--no-index` option for python libraries installs those optimized for the compute canada cluster.\n",
    "\n",
    "**CheckM** requires precalculated data files, which we download to a directory recognized by the tool.\n",
    "\n",
    "``` sh\n",
    "cd ~\n",
    "virtualenv --no-download checkm_genome_env\n",
    "\n",
    "source ~/checkm_genome_env/bin/activate\n",
    "pip install --no-index pysam\n",
    "pip install --no-index checkm_genome\n",
    "\n",
    "# unpack precalculated data files\n",
    "mkdir -p /home/mdprieto/checkm_genome_env/data \n",
    "cd /home/mdprieto/checkm_genome_env/data\n",
    "wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz\n",
    "tar -xzf checkm_data_2015_01_16.tar.gz\n",
    "\n",
    "# tell program where data was unpacked\n",
    "export CHECKM_DATA_PATH=/home/mdprieto/checkm_genome_env/data\n",
    "checkm data setRoot /home/mdprieto/checkm_genome_env/data\n",
    "```\n",
    "\n",
    "Run job\n",
    "\n",
    "``` sh\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --mem-per-cpu=8G #  GB of memory per cpu core\n",
    "#SBATCH --time=04:00:00\n",
    "#SBATCH --ntasks=1 # tasks in parallel\n",
    "#SBATCH --cpus-per-task=12 # CPU cores per task\n",
    "#SBATCH --job-name=\"assembly_qc_checkm\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --output=checkm_hilliam.out\n",
    "\n",
    "###################################     preparation ##############################\n",
    "\n",
    "module load python/3.10.2 scipy-stack   # load python dependencies\n",
    "module load pplacer/1.1.alpha19 prodigal/2.6.3 hmmer/3.2.1 # load other dependencies\n",
    "source ~/checkm_genome_env/bin/activate # activate environment with checkm\n",
    "contigs_dir=\"/scratch/mdprieto/results_hilliam/sample_contigs\" # path to dir with assemblies\n",
    "\n",
    "# make dir for results and save PATH into variable\n",
    "mkdir -p /scratch/mdprieto/results_hilliam/checkm\n",
    "output_dir=\"/scratch/mdprieto/results_hilliam/checkm\"\n",
    "\n",
    "# select marker set for P. aeruginosa\n",
    "checkm taxon_set species 'Pseudomonas aeruginosa' $output_dir/pseudomonas.ms\n",
    "date\n",
    "\n",
    "##################################   analyze  #################################\n",
    "\n",
    "# analyze completeness and contamination of 190 assemblies\n",
    "checkm analyze \\\n",
    "        $output_dir/pseudomonas.ms `#file with checkm marker set for assemblies` \\\n",
    "        $contigs_dir `#dir with assemblies in fasta format` \\\n",
    "        $output_dir `#output directory` \\\n",
    "        -x fa `#extension of assemblies` \\\n",
    "        -t 12 `#number of threads for parallel processing`\n",
    "date\n",
    "\n",
    "# produce summary\n",
    "checkm qa \\\n",
    "        $output_dir/pseudomonas.ms `#file with checkm marker set for assemblies` \\\n",
    "        $output_dir `#output directory` \\\n",
    "        --file $output_dir/checkm_output.tsv \\\n",
    "        --tab_table \\\n",
    "        --threads 12\n",
    "date\n",
    "```\n",
    "\n",
    "## BLAST of pathogen associated genes\n",
    "\n",
    "The pathogen associated genes are stored in a file in the git repo (`burkholderia_pseudomonas_pags.tx`).\n",
    "\n",
    "We use `awk` to extract only the pathogen associated genes (PAGs) of *Pseudomonas* spp.\n",
    "\n",
    "In the code, `NR==1` signals awk to extract the first line with the headers, `$2 ~ /Pseudomonas aeruginosa/ && $5 ~ /pathogen/` matches string to columns, and `-F '\\t'` specifies that the file is tab delimited. We then use cut, tail and sort to extract the accession numbers, eliminate headers and keep only unique identifiers respectively.\n",
    "\n",
    "``` sh\n",
    "# change to the git folder with all primary input\n",
    "cd /home/mdprieto/git/hilliam_pseudomonas_2022/\n",
    "\n",
    "awk -F '\\t' 'NR==1 || ( $2 ~ /Pseudomonas/ && $5 ~ /pathogen/)' burkholderia_pseudomonas_pags.txt | \\\n",
    "    cut -f 3 | \\\n",
    "    tail -n +2 | \\\n",
    "    sort -u > accession_pags.txt\n",
    "```\n",
    "\n",
    "We also use the NCBI e-utilities in order to get the aminoacid sequences for each of the PAGs proteins in fasta format. A collaborator provided two additional fasta files with sequences of proteins of interest, so we add them to our main file before running BLAST.\n",
    "\n",
    "``` sh\n",
    "# install ncbi E-utilities in home dir\n",
    "cd ~ | sh -c \"$(curl -fsSL ftp://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh)\"\n",
    "\n",
    "# save unique PAGs in a new txt file\n",
    "/home/mdprieto/edirect/epost \\\n",
    "    -db protein \\\n",
    "    -format acc \\\n",
    "    -input /home/mdprieto/git/hilliam_pseudomonas_2022/accession_pags.txt | \\\n",
    "    /home/mdprieto/edirect/efetch \\\n",
    "    -format fasta > /home/mdprieto/git/hilliam_pseudomonas_2022/pags.fasta &\n",
    "```\n",
    "\n",
    "### BLAST PAG in newly assembled genomes\n",
    "\n",
    "In order to run BLAST+ against our assembly files, we need to transform each assembly into a database that can be searched by BLAST. Thus, we create a list of all the contigs files from the resulting assemblies and create a BLAST database for each. Finally, we merge all these databases into a single one using `blastdb_aliastool`\n",
    "\n",
    "``` sh\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --mem-per-cpu=10G #  GB of memory per cpu core\n",
    "#SBATCH --time=00:30:00\n",
    "#SBATCH --ntasks=1 # tasks in parallel\n",
    "#SBATCH --cpus-per-task=1 # CPU cores per task\n",
    "#SBATCH --job-name=\"blast_preparation\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --output=blast_preparation.out\n",
    "\n",
    "###########################################################################\n",
    "################################ preparation\n",
    "\n",
    "# load blast+ module\n",
    "module purge\n",
    "module load StdEnv/2020  gcc/9.3.0 blast+/2.12.0\n",
    "\n",
    "# create pathway variables\n",
    "blast_db=\"/scratch/mdprieto/results_hilliam/blastdb\"\n",
    "contigs_dir=\"/scratch/mdprieto/results_hilliam/sample_contigs\"\n",
    "\n",
    "\n",
    "# ---------------- add isolate ID to each contig\n",
    "# finds sequence headers starting with > and adds the isolate ID before contig\n",
    "\n",
    "cd $contigs_dir\n",
    "for i in `ls $contigs_dir`\n",
    "    do\n",
    "    isolate=$(echo $i | grep -oE '[0-9]{1,3}-[ABC][0-9]*')\n",
    "    echo $isolate\n",
    "    perl -pi -e \"s/^>/>$isolate\\_/\" $i \n",
    "    head -n 5 $i\n",
    "    done\n",
    "    \n",
    "grep -oE '[0-9]{1,3}-[ABC][0-9]*'\n",
    "\n",
    "# ---------------- make blast database for each genome\n",
    "\n",
    "# create and move to working directory\n",
    "mkdir -p $blast_db\n",
    "cd $blast_db\n",
    "\n",
    "# create individual databases for each sample_contig\n",
    "for i in `ls $contigs_dir`\n",
    "    do \n",
    "    assembly=\"$contigs_dir/$i\"\n",
    "    echo $assembly\n",
    "    makeblastdb \\\n",
    "        -dbtype nucl \\\n",
    "        -in $assembly \\\n",
    "        -out $blast_db/$i.nt \\\n",
    "        -parse_seqids \\\n",
    "        -title \"$i_blast_database\"\n",
    "    done\n",
    "\n",
    "# ---------------- create unified database for all sample contigs\n",
    "\n",
    "# lists all blast db in folder with output of path only\n",
    "blastdbcmd -list $blast_db -list_outfmt '%f' > blast_databases.txt \n",
    "\n",
    "# now, given the text file with all databases, it creates a virtual database merging all\n",
    "blastdb_aliastool \\\n",
    "    -dblist_file $blast_db/blast_databases.txt \\\n",
    "    -dbtype nucl \\\n",
    "    -title \"hilliam_pseudomonas_assemblies\" \\\n",
    "    -out $blast_db/hilliam_assemblies\n",
    "    \n",
    "```\n",
    "\n",
    "In the following script we will run BLAST+ to align the known PAGs to the genome assemblies from an external dataset (Hilliam-2017). Using the merged database created in the previous step, we run tblastn (protein to nucleotide) using as query (to search) sequences a fasta file containing the protein sequence for all PAGs.\n",
    "\n",
    "``` sh\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --mem-per-cpu=4G #  GB of memory per cpu core\n",
    "#SBATCH --time=00:15:00\n",
    "#SBATCH --ntasks=1 # tasks in parallel\n",
    "#SBATCH --cpus-per-task=4 # CPU cores per task\n",
    "#SBATCH --job-name=\"blast_pags_hilliam\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --output=blast_pags_hilliam.out\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# load blast+ module\n",
    "module purge\n",
    "module load StdEnv/2020  gcc/9.3.0 blast+/2.12.0\n",
    "    \n",
    "tblastn -query /home/mdprieto/git/hilliam_pseudomonas_2022/pags.fasta \\\n",
    "    -db /scratch/mdprieto/results_hilliam/blastdb/hilliam_assemblies \\\n",
    "    -show_gis \\\n",
    "    -outfmt \"7\" \\\n",
    "    -out /scratch/mdprieto/hilliam_blast_full.txt \\\n",
    "    -evalue 1e-50 \\\n",
    "    -num_threads 4 \\\n",
    "    -max_hsps 1\n",
    "\n",
    "# blast of additional proteins by collaborator (Patrick)\n",
    "blastn \\\n",
    "    -query /home/mdprieto/git/hilliam_pseudomonas_2022/patrick_pags.fasta \\\n",
    "    -db /scratch/mdprieto/results_hilliam/blastdb/hilliam_assemblies \\\n",
    "    -show_gis \\\n",
    "    -outfmt \"7\" \\\n",
    "    -out /scratch/mdprieto/patrick_blast.txt \\\n",
    "    -evalue 1e-50 \\\n",
    "    -num_threads 4 \\\n",
    "    -max_hsps 1 \n",
    "```\n",
    "\n",
    "Finally, we move the results to our git repo directory and produce clean versions with headers.\n",
    "\n",
    "``` sh\n",
    "# copy to git repo for project\n",
    "cd ~/scratch\n",
    "cp patrick_blast.txt hilliam_blast_full.txt ~/git/hilliam_pseudomonas_2022/results/\n",
    "\n",
    "# clean format and add headers\n",
    "cd ~/git/hilliam_pseudomonas_2022/results/\n",
    "grep --invert-match \"^#\" hilliam_blast_full.txt | \\\n",
    "    sed '1s/^/qseqid\\tsseqid\\tpiden\\tlength\\tmismatch\\tgapopen\\tqstart\\tqend\\tsstart\\tsend\\tevalue\\tbitscore\\n/' \\\n",
    "    > hilliam_blast_clean.txt\n",
    "\n",
    "grep --invert-match \"^#\" patrick_blast.txt | \\\n",
    "    sed '1s/^/qseqid\\tsseqid\\tpiden\\tlength\\tmismatch\\tgapopen\\tqstart\\tqend\\tsstart\\tsend\\tevalue\\tbitscore\\n/' \\\n",
    "    > patrick_blast_clean.txt   \n",
    "```\n",
    "\n",
    "### Analysis of functional groups in BLAST hits\n",
    "\n",
    "To select optimal candidates for in-vitro evaluation in Aim2, we look for PAGs with transcriptional regulator activity among our hits.\n",
    "\n",
    "In our local machine, I set up the capacity to run multiple searches in `InterProScan` database. Requires installation of Java and running the `Install certificates.command` for our python version.\n",
    "\n",
    "Using `vim`, I pasted the IDs of the PAGs that were found in all the cohort (96 patients) and saved it in `list_pags_all_samples.txt`. Then, using the `seqtk` utility, I create a new fasta file including only these interesting proteins.\n",
    "\n",
    "``` sh\n",
    "# work inside an environment and install dependencies\n",
    "conda create -name interpro\n",
    "conda activate interpro\n",
    "pip3 install xmltramp2 requests\n",
    "\n",
    "# install EMBI REST handler and interproscan script\n",
    "git clone https://github.com/ebi-wp/webservice-clients.git\n",
    "wget https://raw.githubusercontent.com/ebi-wp/webservice-clients/master/python/iprscan5.py\n",
    "\n",
    "# one liner to extract 30 sequences in a file (limit for InterPro)\n",
    "awk \"/^>/ {n++} n>30 {exit} {print}\" input_fasta > output_fasta\n",
    "\n",
    "# new fasta with only PAGs found in all patients\n",
    "seqtk subseq pags.fasta list_pags_all_samples.txt > pags_all_samples.fa\n",
    "\n",
    "# run search\n",
    "# options to output tsv, name output file, and reduce verbosity\n",
    "python3 iprscan5.py \\\n",
    "    --sequence hilliam_pseudomonas_2022/pags_all_samples.fa \\\n",
    "    --email azmigueldario@gmail.com \\\n",
    "    --outformat tsv \\\n",
    "    --outfile results_interpro.tsv \\\n",
    "    --quiet &\n",
    "  \n",
    "# close environment once finished\n",
    "conda deactivate interpro\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bbd84-144c-4f59-aa8b-5aa1433d85d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939629db-dce6-48d8-bb7f-7ea5dd7a5782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9f214-9977-4ba0-a712-e5a2a2702f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
