{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "113df914-b625-46a3-8b82-f522c98f48d8",
   "metadata": {},
   "source": [
    "# Genome assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff02db-aca0-4676-9da3-330584a10d49",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Required tools\n",
    "\n",
    "After the previous steps of quality control (QC), we have reads still in raw_format but now we have a summary of their quality. Furthermore, we have removed regions with poor quality of sequencing (where we cannot be sure if the assigned nucleotides are right) and we removed the adaptor sequences that are added to our DNA for sequencing. \n",
    "\n",
    "In this series of steps, we will do assembly of the reads using a tool called `shovill`. Once again, we will mimic how to run the commands in the **Compute Canada (CC)** cluster of analysis. \n",
    "\n",
    "For these tutorials, tools will be made available using singularity containers, which can be run using the command `singularity run tool_image`. These tools have been made available in the environment already, so there is no need to download them.\n",
    "\n",
    "Tools used in this tutorial:\n",
    "- shovill\n",
    "- singularity\n",
    "\n",
    "We will first explore the structure of our environment and the folders available. Tools downloaded for the tutorial are in the `tools` folder and in the `tutorials` directory are the primary datasets as well as the results of our runs. \n",
    "\n",
    "```\n",
    ".\n",
    "|-- tools\n",
    "`-- tutorials\n",
    "    |-- raw_reads\n",
    "    |-- results_qc\n",
    "    `-- trimmed_reads\n",
    "```\n",
    "\n",
    "For downstream assembly, we will use the curated reads contained in the `trimmed_reads` subdirectory (n=20 files containing paired reads for 10 isolates of _P.aeruginosa_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7787d504-aa7f-47d3-b3c7-00308ecf43c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "ls tutorials/trimmed_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a79a81-00a5-4cc9-9a72-dab9f7c26fd4",
   "metadata": {},
   "source": [
    "## Shovill\n",
    "\n",
    "Shovill is a tool that optimizes the assembler `Spades` to minimize the run time, while maintaining the quality of assembly. See the GitHub repositories of [shovill](https://github.com/tseemann/shovill) and [SPAdes](https://github.com/ablab/spades) for more details. \n",
    "\n",
    "Shovill is not available as a module pre-installed in **CC**, so we must use another strategy. The easiest one is to use a container, \n",
    "We can install from a Docker container. In a HPC, we can create a singularity container from Docker. Singularity is optimized for clusters, Docker modifies root privileges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0295f341-5926-4e79-91c9-d98631079b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671584b-1383-4c64-b835-07d7ee16a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Assembly pipeline with Shovill\n",
    "\n",
    "\n",
    "```sh\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --mem=8gb # 8 GB of memory\n",
    "#SBATCH --time=00:50:00\n",
    "#SBATCH --job-name= singularity build of shovill assembler\n",
    "#SBATCH --chdir= /home/mdprieto/scratch/\n",
    "\n",
    "# load singularity to transform docker container\n",
    "module load singularity/3.8\n",
    "\n",
    "# create singularity container locally\n",
    "singularity build shovill.sif docker://staphb/shovill:latest\n",
    "singularity exec shovill.sif shovill --help\n",
    "\n",
    "# mount my filesystem inside container\n",
    "# ------------------- localscratch is defined to use compute node temp folder\n",
    "singularity run -B /home -B /project -B /scratch -B /localscratch:/temp\n",
    "```\n",
    "After having the singularity container ready, we can assemble our genomes. \n",
    "\n",
    "\n",
    "### Tips to run assembly jobs on Cedar (CC)\n",
    "\n",
    "- Assembly is a resource intensive job that requires that the data is available in memory for processing. So it is necessary to allocate enough ram per CPU to handle the size of each genome. \n",
    "- Also, a part of the available memory should be saved (~4GB) for additional processes or the OS. \n",
    "- Finally, bioinformatic procedures usually use multiple threads to optimize performance, so their efficiency increases with the number of available cores. \n",
    "- In shovill, the `--ram` option specifies the available ram per thread (core)\n",
    "- **Spades performance increases drastically with the number of threads (--cpus-per-task)**\n",
    "- Spades will take input of RAM from shovill as total available mem, better to input limit manually with `--opts \"-m XX\"`\n",
    "\n",
    "\n",
    "```sh\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --mem-per-cpu=12G #  GB of memory per cpu core\n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --ntasks=1 # tasks in parallel\n",
    "#SBATCH --cpus-per-task=16 # CPU cores per task\n",
    "#SBATCH --job-name=\"shovill_assembly_hilliam\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --output=slurm_shovill_16x12.out\n",
    "#SBATCH --mail-user=mprietog@sfu.ca\n",
    "#SBATCH --mail-type=END\n",
    "\n",
    "################################## preparation #########################################\n",
    "\n",
    "# load singularity to execute shovill\n",
    "module purge\n",
    "module load singularity/3.8\n",
    "\n",
    "# mount my filesystem inside container\n",
    "# ---------- localscratch is defined to use compute node temp folder\n",
    "BIND_MOUNT=\"-B /home -B /project -B /scratch -B /localscratch -B /localscratch:/temp\"\n",
    "\n",
    "# create variables and output dir\n",
    "mkdir -p /scratch/mdprieto/results_hilliam/shovill\n",
    "OUTPUT_DIR=\"/scratch/mdprieto/results_hilliam/shovill\"\n",
    "INPUT_DIR=\"/project/6056895/mdprieto/hilliam_pseudomonas/bronchiectasis_reads\"\n",
    "\n",
    "################################## shovill #########################################\n",
    "\n",
    "for file1 in $(ls $INPUT_DIR/*R1*fastq.gz)\n",
    "\n",
    "do\n",
    "    # create environment variables for R2 and R0 files and establish a name for the output directory\n",
    "    file2=${file1/R1/R2}\n",
    "    trimmed=${file1/R1/R0}\n",
    "    out_dir_sample=$(echo $file1 | grep -oE '[0-9]{1,3}-[ABC][0-9]*')\n",
    "\n",
    "    # ------ Execute shovill inside singularity container\n",
    "    # --opts = options to pass into spades assembler\n",
    "    # --ram = total ram in all CPUs\n",
    "\n",
    "    singularity exec $BIND_MOUNT shovill.sif shovill --R1 $file1 --R2 $file2 \\\n",
    "    --outdir $OUTPUT_DIR/$out_dir_sample \\\n",
    "    --opts \"-s $trimmed\" \\\n",
    "    --cpus $SLURM_CPUS_PER_TASK \\\n",
    "    --ram 140 \\\n",
    "    --tmpdir /scratch/mdprieto/tmp\n",
    "    echo \"Finished assembly of sample\"\n",
    "done\n",
    "\n",
    "######################## create new dir with assemblies #############################\n",
    "\n",
    "contigs_dir=\"/scratch/mdprieto/results_hilliam/sample_contigs\"\n",
    "\n",
    "# new directory with sample name appended to the contigs\n",
    "# finds 'contigs.fa' filenames downstream\n",
    "# appends 'sample_name' to each 'contigs.fa' in a new folder 'sample_contigs'\n",
    "\n",
    "mkdir -p $contigs_dir\n",
    "for i in `find /scratch/mdprieto/results_hilliam/shovill -name \"contigs.fa\"`\n",
    "   do cp -n $i $contigs_dir/`echo $i| awk -F/ '{print $6 \"_\" $7}' `\n",
    "done\n",
    "\n",
    "```\n",
    "\n",
    "## QC of assembly\n",
    "\n",
    "### Quast \n",
    "\n",
    "We will use QUAST to generate genome assembly metrics. Before running it, we need to download the reference genome for Pseudomonas aeruginosa (PA1). The output directory is specified with the option `-P`\n",
    "\n",
    "```sh\n",
    "\n",
    "# genomic fasta\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/496/605/GCF_000496605.2_ASM49660v2/GCF_000496605.2_ASM49660v2_genomic.fna.gz -P /project/6056895/mdprieto/hilliam_pseudomonas/pseudomonas_pa1_reference\n",
    "\n",
    "# genomic coordinates annotation\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/496/605/GCF_000496605.2_ASM49660v2/GCF_000496605.2_ASM49660v2_genomic.gff.gz -P /project/6056895/mdprieto/hilliam_pseudomonas/pseudomonas_pa1_reference\n",
    "```\n",
    "Now, we run quast with and without reference genome. With reference genome we obtain basic assembly measures and metrics of coverage against the curated assembly. Without a reference, we would obtain only the basic measures. \n",
    "\n",
    "```sh\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --mem-per-cpu=4G #  GB of memory per cpu core\n",
    "#SBATCH --time=00:30:00\n",
    "#SBATCH --ntasks=1 # tasks in parallel\n",
    "#SBATCH --cpus-per-task=8 # CPU cores per task\n",
    "#SBATCH --job-name=\"quast_hilliam\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --output=quast_hilliam.out\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# ----------------------- preparation\n",
    "\n",
    "# load QUAST module and dependencies\n",
    "module load StdEnv/2020 gcc/9.3.0 quast/5.0.2\n",
    "\n",
    "# define internal variables\n",
    "genome_fasta=\"/project/6056895/mdprieto/hilliam_pseudomonas/pseudomonas_pa1_reference/GCF_000496605.2_ASM49660v2_genomic.fna.gz\"\n",
    "genome_gff=\"/project/6056895/mdprieto/hilliam_pseudomonas/pseudomonas_pa1_reference/GCF_000496605.2_ASM49660v2_genomic.gff.gz\"\n",
    "contigs_dir=\"/scratch/mdprieto/results_hilliam/sample_contigs\"\n",
    "output_dir=\"/scratch/mdprieto/results_hilliam/quast\"\n",
    "\n",
    "# ----------------------- quast no reference genome\n",
    "\n",
    "quast.py $contigs_dir/*.fa \\\n",
    "\t\t\t-r $genome_fasta \\\n",
    "\t\t\t-g $genome_gff \\\n",
    "\t\t\t-o $output_dir \\\n",
    "\t\t\t--threads 7\n",
    "\n",
    "```\n",
    "\n",
    "### CheckM\n",
    "\n",
    "**CheckM** infers the quality of the genome assembly based on the presence and uniqueness of these sets of gene markers. It determines the completeness (coverage of reference genome) and the contamination of the input draft genomes.\n",
    "\n",
    "**CheckM** is not available in the CC cluster. To install it, we create a virtual environment of python in our home directory. After loading the interpreter, we load the `scipy-stack` module that contains necessary python dependencies (matplotlib and numpy). Also, we load a set of bioinformatic tools dependencies (pplacer, prodigal and hmmer). \n",
    "\n",
    "```sh\n",
    "module load python/3.10.2 scipy-stack\n",
    "module load pplacer/1.1.alpha19 prodigal/2.6.3 hmmer/3.2.1\n",
    "```\n",
    "Is best practice to create virtual environment in your home or project directory. I create `checkm_genome_env` in the home dir; python dependencies are installed after loading environment. The `--no-index` option for python libraries installs those optimized for the compute canada cluster. \n",
    "\n",
    "__CheckM__ requires precalculated data files, which we download to a directory recognized by the tool. \n",
    "\n",
    "```sh\n",
    "cd ~\n",
    "virtualenv --no-download checkm_genome_env\n",
    "\n",
    "source ~/checkm_genome_env/bin/activate\n",
    "pip install --no-index pysam\n",
    "pip install --no-index checkm_genome\n",
    "\n",
    "# unpack precalculated data files\n",
    "mkdir -p /home/mdprieto/checkm_genome_env/data \n",
    "cd /home/mdprieto/checkm_genome_env/data\n",
    "wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz\n",
    "tar -xzf checkm_data_2015_01_16.tar.gz\n",
    "\n",
    "# tell program where data was unpacked\n",
    "export CHECKM_DATA_PATH=/home/mdprieto/checkm_genome_env/data\n",
    "checkm data setRoot /home/mdprieto/checkm_genome_env/data\n",
    "```\n",
    "\n",
    "Run job\n",
    "\n",
    "```sh\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --mem-per-cpu=8G #  GB of memory per cpu core\n",
    "#SBATCH --time=04:00:00\n",
    "#SBATCH --ntasks=1 # tasks in parallel\n",
    "#SBATCH --cpus-per-task=12 # CPU cores per task\n",
    "#SBATCH --job-name=\"assembly_qc_checkm\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --output=checkm_hilliam.out\n",
    "\n",
    "###################################     preparation ##############################\n",
    "\n",
    "module load python/3.10.2 scipy-stack   # load python dependencies\n",
    "module load pplacer/1.1.alpha19 prodigal/2.6.3 hmmer/3.2.1 # load other dependencies\n",
    "source ~/checkm_genome_env/bin/activate # activate environment with checkm\n",
    "contigs_dir=\"/scratch/mdprieto/results_hilliam/sample_contigs\" # path to dir with assemblies\n",
    "\n",
    "# make dir for results and save PATH into variable\n",
    "mkdir -p /scratch/mdprieto/results_hilliam/checkm\n",
    "output_dir=\"/scratch/mdprieto/results_hilliam/checkm\"\n",
    "\n",
    "# select marker set for P. aeruginosa\n",
    "checkm taxon_set species 'Pseudomonas aeruginosa' $output_dir/pseudomonas.ms\n",
    "date\n",
    "\n",
    "##################################   analyze  #################################\n",
    "\n",
    "# analyze completeness and contamination of 190 assemblies\n",
    "checkm analyze \\\n",
    "        $output_dir/pseudomonas.ms `#file with checkm marker set for assemblies` \\\n",
    "        $contigs_dir `#dir with assemblies in fasta format` \\\n",
    "        $output_dir `#output directory` \\\n",
    "        -x fa `#extension of assemblies` \\\n",
    "        -t 12 `#number of threads for parallel processing`\n",
    "date\n",
    "\n",
    "# produce summary\n",
    "checkm qa \\\n",
    "        $output_dir/pseudomonas.ms `#file with checkm marker set for assemblies` \\\n",
    "        $output_dir `#output directory` \\\n",
    "        --file $output_dir/checkm_output.tsv \\\n",
    "        --tab_table \\\n",
    "        --threads 12\n",
    "date\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## BLAST of pathogen associated genes\n",
    "\n",
    "The pathogen associated genes are stored in a file in the git repo (`burkholderia_pseudomonas_pags.tx`). \n",
    "\n",
    "We use `awk` to extract only the pathogen associated genes (PAGs) of *Pseudomonas* spp. \n",
    "\n",
    "In the code, `NR==1` signals awk to extract the first line with the headers, `$2 ~ /Pseudomonas aeruginosa/ && $5 ~ /pathogen/` matches string to columns, and `-F '\\t'` specifies that the file is tab delimited. We then use cut, tail and sort to extract the accession numbers, eliminate headers and keep only unique identifiers respectively. \n",
    "\n",
    "```sh\n",
    "# change to the git folder with all primary input\n",
    "cd /home/mdprieto/git/hilliam_pseudomonas_2022/\n",
    "\n",
    "awk -F '\\t' 'NR==1 || ( $2 ~ /Pseudomonas/ && $5 ~ /pathogen/)' burkholderia_pseudomonas_pags.txt | \\\n",
    "\tcut -f 3 | \\\n",
    "\ttail -n +2 | \\\n",
    "\tsort -u > accession_pags.txt\n",
    "\n",
    "``` \n",
    "\n",
    "We also use the NCBI e-utilities in order to get the aminoacid sequences for each of the PAGs proteins in fasta format. A collaborator provided two additional fasta files with sequences of proteins of interest, so we add them to our main file before running BLAST.\n",
    "\n",
    "```sh\n",
    "# install ncbi E-utilities in home dir\n",
    "cd ~ | sh -c \"$(curl -fsSL ftp://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh)\"\n",
    "\n",
    "# save unique PAGs in a new txt file\n",
    "/home/mdprieto/edirect/epost \\\n",
    "\t-db protein \\\n",
    "\t-format acc \\\n",
    "\t-input /home/mdprieto/git/hilliam_pseudomonas_2022/accession_pags.txt | \\\n",
    "\t/home/mdprieto/edirect/efetch \\\n",
    "\t-format fasta > /home/mdprieto/git/hilliam_pseudomonas_2022/pags.fasta &\n",
    "\n",
    "```\n",
    "\n",
    "### BLAST PAG in newly assembled genomes\n",
    "\n",
    "In order to run BLAST+ against our assembly files, we need to transform each assembly into a database that can be searched by BLAST. Thus, we create a list of all the contigs files from the resulting assemblies and create a BLAST database for each. Finally, we merge all these databases into a single one using `blastdb_aliastool` \n",
    "\n",
    "```sh\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --mem-per-cpu=10G #  GB of memory per cpu core\n",
    "#SBATCH --time=00:30:00\n",
    "#SBATCH --ntasks=1 # tasks in parallel\n",
    "#SBATCH --cpus-per-task=1 # CPU cores per task\n",
    "#SBATCH --job-name=\"blast_preparation\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --output=blast_preparation.out\n",
    "\n",
    "###########################################################################\n",
    "################################ preparation\n",
    "\n",
    "# load blast+ module\n",
    "module purge\n",
    "module load StdEnv/2020  gcc/9.3.0 blast+/2.12.0\n",
    "\n",
    "# create pathway variables\n",
    "blast_db=\"/scratch/mdprieto/results_hilliam/blastdb\"\n",
    "contigs_dir=\"/scratch/mdprieto/results_hilliam/sample_contigs\"\n",
    "\n",
    "\n",
    "# ---------------- add isolate ID to each contig\n",
    "# finds sequence headers starting with > and adds the isolate ID before contig\n",
    "\n",
    "cd $contigs_dir\n",
    "for i in `ls $contigs_dir`\n",
    "\tdo\n",
    "\tisolate=$(echo $i | grep -oE '[0-9]{1,3}-[ABC][0-9]*')\n",
    "\techo $isolate\n",
    "\tperl -pi -e \"s/^>/>$isolate\\_/\" $i \n",
    "\thead -n 5 $i\n",
    "\tdone\n",
    "\t\n",
    "grep -oE '[0-9]{1,3}-[ABC][0-9]*'\n",
    "\n",
    "# ---------------- make blast database for each genome\n",
    "\n",
    "# create and move to working directory\n",
    "mkdir -p $blast_db\n",
    "cd $blast_db\n",
    "\n",
    "# create individual databases for each sample_contig\n",
    "for i in `ls $contigs_dir`\n",
    "\tdo \n",
    "\tassembly=\"$contigs_dir/$i\"\n",
    "\techo $assembly\n",
    "\tmakeblastdb \\\n",
    "\t\t-dbtype nucl \\\n",
    "\t\t-in $assembly \\\n",
    "\t\t-out $blast_db/$i.nt \\\n",
    "\t\t-parse_seqids \\\n",
    "\t\t-title \"$i_blast_database\"\n",
    "\tdone\n",
    "\n",
    "# ---------------- create unified database for all sample contigs\n",
    "\n",
    "# lists all blast db in folder with output of path only\n",
    "blastdbcmd -list $blast_db -list_outfmt '%f' > blast_databases.txt \n",
    "\n",
    "# now, given the text file with all databases, it creates a virtual database merging all\n",
    "blastdb_aliastool \\\n",
    "\t-dblist_file $blast_db/blast_databases.txt \\\n",
    "\t-dbtype nucl \\\n",
    "\t-title \"hilliam_pseudomonas_assemblies\" \\\n",
    "\t-out $blast_db/hilliam_assemblies\n",
    "\t\n",
    "```\n",
    "\n",
    "In the following script we will run BLAST+ to align the known PAGs to the genome assemblies from an external dataset (Hilliam-2017). Using the merged database created in the previous step, we run tblastn (protein to nucleotide)\n",
    "using as query (to search) sequences a fasta file containing the protein sequence for all PAGs. \n",
    "\n",
    "```sh\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-whsiao-ab\n",
    "#SBATCH --mem-per-cpu=4G #  GB of memory per cpu core\n",
    "#SBATCH --time=00:15:00\n",
    "#SBATCH --ntasks=1 # tasks in parallel\n",
    "#SBATCH --cpus-per-task=4 # CPU cores per task\n",
    "#SBATCH --job-name=\"blast_pags_hilliam\"\n",
    "#SBATCH --chdir=/scratch/mdprieto/\n",
    "#SBATCH --output=blast_pags_hilliam.out\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# load blast+ module\n",
    "module purge\n",
    "module load StdEnv/2020  gcc/9.3.0 blast+/2.12.0\n",
    "\t\n",
    "tblastn -query /home/mdprieto/git/hilliam_pseudomonas_2022/pags.fasta \\\n",
    "\t-db /scratch/mdprieto/results_hilliam/blastdb/hilliam_assemblies \\\n",
    "\t-show_gis \\\n",
    "\t-outfmt \"7\" \\\n",
    "\t-out /scratch/mdprieto/hilliam_blast_full.txt \\\n",
    "\t-evalue 1e-50 \\\n",
    "\t-num_threads 4 \\\n",
    "\t-max_hsps 1\n",
    "\n",
    "# blast of additional proteins by collaborator (Patrick)\n",
    "blastn \\\n",
    "\t-query /home/mdprieto/git/hilliam_pseudomonas_2022/patrick_pags.fasta \\\n",
    "\t-db /scratch/mdprieto/results_hilliam/blastdb/hilliam_assemblies \\\n",
    "\t-show_gis \\\n",
    "\t-outfmt \"7\" \\\n",
    "\t-out /scratch/mdprieto/patrick_blast.txt \\\n",
    "\t-evalue 1e-50 \\\n",
    "\t-num_threads 4 \\\n",
    "\t-max_hsps 1 \n",
    "```\n",
    "Finally, we move the results to our git repo directory and produce clean versions with headers.\n",
    "\n",
    "```sh\n",
    "# copy to git repo for project\n",
    "cd ~/scratch\n",
    "cp patrick_blast.txt hilliam_blast_full.txt ~/git/hilliam_pseudomonas_2022/results/\n",
    "\n",
    "# clean format and add headers\n",
    "cd ~/git/hilliam_pseudomonas_2022/results/\n",
    "grep --invert-match \"^#\" hilliam_blast_full.txt | \\\n",
    "\tsed '1s/^/qseqid\\tsseqid\\tpiden\\tlength\\tmismatch\\tgapopen\\tqstart\\tqend\\tsstart\\tsend\\tevalue\\tbitscore\\n/' \\\n",
    "\t> hilliam_blast_clean.txt\n",
    "\n",
    "grep --invert-match \"^#\" patrick_blast.txt | \\\n",
    "\tsed '1s/^/qseqid\\tsseqid\\tpiden\\tlength\\tmismatch\\tgapopen\\tqstart\\tqend\\tsstart\\tsend\\tevalue\\tbitscore\\n/' \\\n",
    "\t> patrick_blast_clean.txt\t\n",
    "\n",
    "```\n",
    "\n",
    "### Analysis of functional groups in BLAST hits\n",
    "\n",
    "To select optimal candidates for in-vitro evaluation in Aim2, we look for PAGs with transcriptional regulator activity among our hits. \n",
    "\n",
    "In our local machine, I set up the capacity to run multiple searches in `InterProScan` database. Requires installation of Java and running the `Install certificates.command` for our python version.\n",
    "\n",
    "Using `vim`, I pasted the IDs of the PAGs that were found in all the cohort (96 patients) and saved it in `list_pags_all_samples.txt`. Then, using the `seqtk` utility, I create a new fasta file including only these interesting proteins. \n",
    "\n",
    "```sh\n",
    "# work inside an environment and install dependencies\n",
    "conda create -name interpro\n",
    "conda activate interpro\n",
    "pip3 install xmltramp2 requests\n",
    "\n",
    "# install EMBI REST handler and interproscan script\n",
    "git clone https://github.com/ebi-wp/webservice-clients.git\n",
    "wget https://raw.githubusercontent.com/ebi-wp/webservice-clients/master/python/iprscan5.py\n",
    "\n",
    "# one liner to extract 30 sequences in a file (limit for InterPro)\n",
    "awk \"/^>/ {n++} n>30 {exit} {print}\" input_fasta > output_fasta\n",
    "\n",
    "# new fasta with only PAGs found in all patients\n",
    "seqtk subseq pags.fasta list_pags_all_samples.txt > pags_all_samples.fa\n",
    "\n",
    "# run search\n",
    "# options to output tsv, name output file, and reduce verbosity\n",
    "python3 iprscan5.py \\\n",
    "\t--sequence hilliam_pseudomonas_2022/pags_all_samples.fa \\\n",
    "\t--email azmigueldario@gmail.com \\\n",
    "\t--outformat tsv \\\n",
    "\t--outfile results_interpro.tsv \\\n",
    "\t--quiet &\n",
    "  \n",
    "# close environment once finished\n",
    "conda deactivate interpro\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895dac5-d0b2-4a03-a807-84a27c15594a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52af57-9f08-4eb6-b4d7-e521bbb57264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash [conda env:root] *",
   "language": "bash",
   "name": "conda-root-bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
